{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tweets\n",
    "\n",
    "This notebook is for cleaning the tweets and tokenizing with a subsequent analysis.\n",
    "\n",
    "\n",
    "Actually, these guys: https://link.springer.com/chapter/10.1007/978-3-319-09339-0_62 argue that lemmatization and stemming lead to worse performance in terms of sentiment analysis, while features reservation, negation transformation and repeated letters normalization improves it. They also get better results when by using bigrams and emotions features.\n",
    "\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S1877050913001385\n",
    "\n",
    "Also: Stop Word removal might not be necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import swifter\n",
    "import pickle\n",
    "\n",
    "# Text mining packages\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "# Import cleaning function\n",
    "from utils.cleaning import clean_tweet, has_arabic, has_cyrillic\n",
    "\n",
    "reload = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading tweets and user data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already cleaned the tweets a bit checking for user locations and so on. Now it's time to clean the texts, tokenize, remove stop words and so on.\n",
    "\n",
    "```clean_tweet``` does a bunch of stuff, including removal of retweets, hyperlinks, emojis and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload:\n",
    "    tweets = pd.read_pickle(\"../data/tweets_preprocessed.pickle\")\n",
    "    users = pd.read_pickle(\"../data/user_clean.pickle\")\n",
    "    \n",
    "else:\n",
    "    tweets = pd.read_pickle(\"../data/tweets_clean.pickle\")\n",
    "    users = pd.read_pickle(\"../data/user_clean.pickle\")\n",
    "    \n",
    "    # Clean\n",
    "    tweets[\"tweet_text\"] = tweets[\"tweet_text\"].swifter.apply(clean_tweet)\n",
    "    \n",
    "    # Save it, just to make sure nothing gets lost\n",
    "    with open('../data/tweets_preprocessed.pickle', 'wb') as f:\n",
    "        pickle.dump(tweets, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning, we can tokenize. Hashtags should be helpful with getting meaningful embeddings since they contain information about the topic. We could later check whether there are differences in embeddings / clusters with or without hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tweets_tokenized = [tokenizer.tokenize(t) for t in tweets[\"tweet_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tweets_tokenized.pickle', 'wb') as f:\n",
    "    pickle.dump(tweets_tokenized, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
