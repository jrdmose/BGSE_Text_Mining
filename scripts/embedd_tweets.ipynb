{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenized tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tweets_tokenized.pickle', 'rb') as f:\n",
    "    tweets_tokenized = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"../models/w2v_skipgram_w4.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedd tweets\n",
    "\n",
    "- First define a function that adds the individual words in each tweet together to form a vector representation of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function embedds tweets based on the sum of the vector representations of each word in the tweet\n",
    "def embed_w2v_sum(tokens, w2v):\n",
    "    \n",
    "    # Get the index of the word vectors for each token in a tweet\n",
    "    idxs = [w2v.wv.vocab.get(t) for t in tokens]\n",
    "    idxs = [t.index for t in idxs if t]\n",
    "    \n",
    "    # N is the number of tokens in tweet\n",
    "    N = w2v.wv.vectors.shape[1]\n",
    "    if len(idxs) < 1:\n",
    "        return np.zeros(N)\n",
    "    \n",
    "    # A tweet is represented by the sum of the vectors it contains\n",
    "    a = np.sum(w2v.wv.vectors[idxs, :], axis=0) \n",
    "    \n",
    "    # Standardize the whole tweet by its norm\n",
    "    a /= np.linalg.norm(a)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to embedd the tweets\n",
    "tweets_embedded = np.array([embed_w2v_sum(t, w2v_model) for t in tweets_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the full tweet data\n",
    "with open('../data/tweets_preprocessed.pickle', 'rb') as f:\n",
    "    tweet_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some unwanted stuff\n",
    "cols_to_keep = [\"userid\", \"user_screen_name\",\"user_display_name\",\"follower_count\",\n",
    "                \"tweet_text\",\"is_retweet\",\"retweet_count\",\"hashtags\"]\n",
    "\n",
    "tweet_df = tweet_df.loc[:,cols_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embedded tweets as new column in data frame\n",
    "cols = [str(i) + \"_dim\" for i in range(0,tweets_embedded.shape[1])]\n",
    "\n",
    "# Embedding df\n",
    "embedding_df = pd.DataFrame(tweets_embedded,columns = cols, index= tweet_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets_embedded\n",
    "\n",
    "import garba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join and save\n",
    "tweets_final = tweet_df.join(embedding_df)\n",
    "\n",
    "# Pickle - Tweets\n",
    "with open('../data/tweets_final.pickle', 'wb') as f:\n",
    "    pickle.dump(tweets_final, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Cluster Embedded Tweets\n",
    "\n",
    "Let's start with taking some very interesting and/or offensive tweets and finding the most similar as well as dissimilar tweets to them.\n",
    "\n",
    "We should also work on a subset, so let's take tweets with a higher number of retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some tweets first. We pick out two important accounts: KaniJJackson and Pamela_Moore13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.comparison import embedd_tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the corresponding subsets of the tweet df and embedded tweets, we will work with those now\n",
    "subset = tweets_final[(tweets_final.user_screen_name == \"KaniJJackson\") | (tweets_final.user_screen_name == \"Pamela_Moore13\")].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize this subset, maybe we already get a separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns of embeddings\n",
    "X = subset.loc[:,subset.columns[-100:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedd\n",
    "d = embedd_tSNE(X,pca_n =10, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset[\"x\"] = d[:,0]\n",
    "subset[\"y\"] = d[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=subset, x= \"x\", y=\"y\", hue = \"user_screen_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this is really anything but a separation. On the other hand, project these tweets in two dimensions is really hard. t-SNE is really not supposed to be used for clustering, rather for visualization. Further, I think t-SNE picks up the hashtags (as intended to) which can be shared across users. Let's verify this quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subsets of tweets by hashtag\n",
    "subset = tweets_final[(tweets_final.hashtags == \"[MAGA]\") | (tweets_final.hashtags == \"[BlackLivesMatter]\")].copy()\n",
    "\n",
    "# Get columns of embeddings\n",
    "X = subset.loc[:,subset.columns[-100:]].values\n",
    "\n",
    "# Embedd\n",
    "d = embedd_tSNE(X,pca_n =10, metric='cosine')\n",
    "\n",
    "subset[\"x\"] = d[:,0]\n",
    "subset[\"y\"] = d[:,1]\n",
    "\n",
    "# Visualize\n",
    "sns.scatterplot(data=subset, x= \"x\", y=\"y\", hue = \"hashtags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, MAGA seems to be one single cluster, whereas BlackLivesMatter consists of many groups. Maybe we can already see polarization inside the BlackLivesMatter topic. It could be that the Russians tried to shape the discussion about race by pushing two sides.\n",
    "\n",
    "How to continue:\n",
    "\n",
    "- Cluster all tweets, see whether there are meaningful clusters\n",
    "- If yes, explore these\n",
    "- If not, explore single topics, try to find clusters within topics, if there are any, explore these.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
